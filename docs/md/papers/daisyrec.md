# Are We Evaluating Rigorously? Benchmarking Recommendation for Reproducible Evaluation and Fair Comparison
私たちは厳密に評価していますか？ 再現性のある評価と公正な比較のためのベンチマークの推奨事項<br/>
https://dl.acm.org/doi/10.1145/3383313.3412489

## 要約
毎年膨大な量の推奨アルゴリズムが提案されており、1つの重要な問題がかなりの注目を集めています。評価のための効果的なベンチマークがないため、再現性のない評価と不公平な比較という2つの大きな懸念が生じます。このホワイトペーパーは、暗黙的なフィードバックベースのトップN推奨アルゴリズムの厳密な（つまり、再現可能で公正な）評価を行うことを目的としています。最初に、8つの一流会議（RecSys、SIGIRなど）で公開された85の推奨論文を体系的にレビューして、重要な評価要素（データ分割やパラメーター調整戦略など）を要約します。全体的な実証研究を通じて、さまざまな要素が次に、推奨パフォーマンスが詳細に分析されます。その後、標準化された手順でベンチマークを作成し、後の調査の参照として、広く使用されている6つのデータセットの6つのメトリックにわたって7つの適切に調整された最先端のパフォーマンスを提供します。さらに、ユーザーフレンドリーなPythonツールキットをリリースしました。これは、推奨のための厳密な評価の広い範囲に対応するという点で、既存のツールキットとは異なります。全体として、私たちの仕事は推奨評価の問題に光を当て、さらなる調査の基礎を築きます。コードとデータセットはGitHub（https://github.com/AmazingDD/dreamsRec）で入手できます。

## 背景
ここ数十年は、学界と産業界の両方でレコメンダーシステムの大きな繁栄を目の当たりにしてきました[32]。パーソナライズされたサービスを顧客に提供することを目的として、推奨アルゴリズムは広く研究され、eコマース（Amazon、Tmallなど）からロケーションベースのソーシャルネットワーク（Foursquare、Yelpなど）やマルチメディア（Foursquare、Yelpなど）に至るまで、さまざまなドメインで適用されています。例：Netflix、Spotify）。既存のアルゴリズムは、メモリベースの方法（MM）[7、29]、潜在因子ベースの方法（LFM）[32]、およびアイテム埋め込みベースの方法を含む表現学習ベースの方法（RLM）の3種類の基礎となる手法によって支配されています。 [2]、および深層学習ベースの方法（DLM）[14、44]。膨大な量の推奨アルゴリズムが提案されているため、コミュニティの研究者から1つの重要な問題が大きな注目を集めています。それは、評価のための効果的なベンチマークがないことです。その結果、それは2つの主要な懸念、すなわち再現不可能な評価と不公正な比較につながります。

これらの問題は[27、28]によって最初に指摘されました。最近の研究[26]は、過去5年間に多数の出版物で報告されたベースラインの結果が最適ではないことを示しています。注意深く設定すると、ベースラインは新しく提案された方法のほとんどを上回ることができます。これは別の最新の研究[7]と一致しており、最近提案されたDLM [14]は、微調整されたパラメーターを使用して、ItemKNN[29]などのより単純なベースラインによって打ち負かされる可能性があることを発見しました。これらの調査結果は、推奨方法の評価に関する活発な議論を引き起こしました。

提案されたアプローチを公正に評価するために成熟したベンチマーク[8]が利用できる他のドメイン、たとえばコンピュータービジョンとは異なり、ベンチマークの推奨は2つの側面でより困難です。（1）各アプリケーションドメインに異なるプラットフォームからのデータセットが多数存在する。たとえば、映画ドメインで広く使用されているデータセットには、MovieLens（ML）、Netflix、AmazonMovieなどがあります。同じデータセットであっても、ML-100K / 1M / 10M / 20M / 25M / 最新 など、さまざまな期間をカバーするさまざまなバージョンが存在する場合があります。研究者がヒューリスティックに異なるデータセットを選択し、選択したデータセットの結果のみを報告するのが一般的です。 （2）さまざまなデータ処理戦略、データ分割方法、評価指標、パラメーター設定などがあります。たとえば、5フィルター設定[41]を採用して、評価が5未満のユーザーやアイテムを除外する研究もあれば、 10フィルター設定を採用[37、38]。データ分割方法に関しては、Leave-one-out [14]を利用するものもあれば、比率による分割を利用するものもあります（たとえば、トレーニング：検証：テスト= 80％：10％：10％）[37]。最も重要なことは、大多数の論文がデータ処理とパラメーター設定の詳細を報告しておらず、さまざまな研究者による再現の結果に一貫性がないことです。これらの問題は、推奨の評価の厳密さ（つまり、再現性と公平性）を調査する動機になります。

推奨で最も重要なタスクの1つであるトップNの推奨タスクに焦点を当て、他のタスク（時間、セッション、場所、グループ、クロスドメイン対応の推奨など）を将来の調査のために残します。これを達成するために、（1）RecSys、KDD、SIGIR、WWWを含む8つのトップティア会議で、最近3年間（2017-2019）に公開された暗黙のフィードバックベースのトップNレコメンデーションに関連する85の論文を最初に体系的にレビューします、IJCAI、AAAI、WSDM、CIKM（高品質のレコメンデーションペーパーを受け入れる最も重要な場所）。これらの論文から、利用されたデータセット、データ前処理戦略、比較ベースライン、損失関数設計、ネガティブサンプリング戦略、データ分割方法、評価メトリック、パラメーター調整戦略など、評価に関連する重要な要素を要約します。 （2）次に、全体的な実証研究を通じて、推奨パフォーマンスに対するさまざまな要因の影響を包括的に分析します。

私たちの結果は、最近の研究で得られた発見をさらに確認しています[7、26]。さらに、いくつかの興味深い観察結果が指摘されています。たとえば、
（a）推奨パフォーマンスはデータセットの密度が高いほど必ずしも向上するわけではありません。<br/>
（b）ペアワイズ対数損失のある目的関数は、一般に他のタイプの目的関数よりも優れています。<br/>
（c）均一サンプラーは、シンプルですが、人気ベースのサンプラーよりも優れたパフォーマンスを発揮します。<br/>
（d）1つの特定のメトリックに最適なハイパーパラメータ設定は、必ずしも最適なw.r.tを保証するものではありません。<br/>
その他の指標。 （3）経験的結果に基づいて、評価の再現性と公平性を向上させるための標準化された手順を提案するベンチマークを作成します。一方、6つのメトリックにわたって広く使用されている6つのデータセットに対する7つの適切に調整された最先端のパフォーマンスは、後の調査の参照として提供されます。 （4）さらに、厳密な評価のための使いやすいPythonツールキットをリリースしました。いくつかの既存のライブラリ（LibRec[11]やDeepRec[45]など）が推奨されていますが、それらは主にさまざまな最先端のロジックを再現することを目的としていることを指摘します。厳密な評価を推奨するという観点から再現性を考えることはめったにありません。要約すると、私たちの仕事は、推奨の評価における厳格さの緊急の問題に光を当て、このトピックに関するさらなる調査の基礎を築きます。

## 論文の収集と分析
推薦のための厳密な評価を達成するために、私たちは最初に、RecSys、KDD、SIGIR、WWW、IJCAI、AAAI、WSDM、CIKMの8つのトップクラスの会議で最近3年間（2017-2019）に公開された論文に対して包括的なレビューを行います。出発点として、私たちは主に、推奨の最も重要なタスクの1つである暗黙のフィードバックベースのトップN推奨の方法に焦点を当てます。他のタスク（セッション認識の推奨など）は、将来の調査のために残されています。まず、3年間で8回の会議について、承認されたフルペーパーリスト（8 ∗ 3 = 24）を検索します。私たちの関心と24のリストを考慮して、「推奨*」または「協調フィルタリング」というキーワードを含むタイトルの論文を検討します。その後、ランキング指標（適合率、再現率など）を採用する論文を手動で選択して、推奨のパフォーマンスを評価します。最後に、85の論文のコレクションを取得し、図1（a）に会議と発行年にわたるそれらの分布を要約します。スペースの制限により、これらのペーパーの詳細は、GitHubリポジトリの追加資料にリストされています。このコレクションに基づいて、使用されたデータセット、データの前処理、比較ベースライン、損失関数の設計、ネガティブサンプリング戦略、データ分割方法、評価指標、パラメーターの調整など、評価に関連する重要な要素を調査するための体系的な分析を行います。以下に詳述する戦略。

### データセット
収集された論文を分析すると、利用されているデータセットの2つの主要な問題が見つかります。
（1）ドメインの多様性。つまり、GitHubリポジトリの追加資料に示されているように、さまざまなドメイン内およびドメイン間でデータセットがたくさんあります。 <br/>
（2）バージョンの多様性。つまり、多くのデータセットは同じ名前ですが、バージョンが異なる場合があります。<br/>
たとえば、ホスティングチャレンジのさまざまなラウンドに合わせて更新されているため、Yelpには3つ以上のバージョンがあります。全体として、85の論文で使用されている65の異なるデータセットがあります（データセットの異なるバージョンは1回だけカウントされます）。図1（b）は、関連する論文の割合で測定した上位15のデータセットの人気を示しています。85の論文の約90％が、15のデータセットの少なくとも1つを採用しています。データセットの人気とドメインカバレッジを考慮して、図1（b）で赤で強調表示されているように、調査対象として6つのデータセットを選択します：ML-1M（映画）、Lastfm（音楽）、Yelp（LBSN）、Epinions（SN）、Book-X （本）とAMZe（消耗品）。選択したデータセットは、コレクションの67％の論文をカバーしています。バージョンの多様性の問題を緩和するために、データソースの権限と情報の豊富さを考慮して慎重に選択します。これは、多様な推奨者に関する調査に役立つ可能性があります。具体的には、GroupLens（grouplens.org/datasets/movielens/）によってリリースされたMovieLens-1M（ML-1M）を使用します。 Lastfmは、第2回国際ワークショップHetRec 2011（ir.ii.uam.es/hetrec2011/）によってリリースされました。 Yelpは2018年にKaggleによって作成されました（www.kaggle.com/yelp-dataset/ yelp-dataset）; Epinionsは、タイムスタンプとアイテムカテゴリ情報を含む[33]によってクロールされました。 Book-Crossing（Book-X）[47]は、Book-Crossingコミュニティ（grouplens.org/datasets/book-crossing/）のCai-NicolasZieglerによって収集されました。 Amazon Electronic（AMZe）は、Julian McAuley（jmcauley.ucsd.edu/data/amazon/links.html）によってリリースされました。すべてのデータセットの統計を表1に示します。

### データの前処理
暗黙的なフィードバックに焦点を当てると、明示的なフィードバック（評価やカウントなど）を持つデータセットは2値化されて暗黙的なデータになります。 u∈U、i∈Iがユーザーuとアイテムiを表すとします。 U、私はユーザーとアイテムのセットです。 rui∈Rは、iに対するuのバイナリフィードバックです。ユーザーごとに、しきい値t以上のすべての明示的なフィードバックを正のフィードバック（rui = 1）に変換します。それ以外の場合は、負のフィードバック（rui = 0）。 tに関しては、論文が異なれば設定も異なる場合があります（t = 1/2/3/4など）。大多数の研究[34、36、39]に従って、ML-1Mにはt=4を設定し、残りのデータセットにはt=1を設定しました。一般に、元のデータセットは非常にまばらであり、ほとんどのユーザーは少数のアイテム（たとえば、5つ未満）のみを操作します。評価では、データセットは通常、非常に非アクティブなユーザーとアイテムを除外するために前処理されます。紙のコレクションを分析したところ、紙の約50％が前処理戦略を採用していることがわかりました。論文の27％は元のデータセットを利用しています。論文の23％は、データ処理の詳細を報告していません。前処理戦略を採用している論文の中で、それらの60％以上が、データセットの5つまたは10のフィルター設定を利用しています。つまり、ユーザーとアイテムの相互作用がそれぞれ5または10未満のフィルターを除外しています。一方、他の人は、データセットに1、2、3、4、20、または30フィルター設定などを採用しています。さまざまなデータスパース性にわたるメソッドのパフォーマンスと堅牢性を測定するために、元のデータセットに加えて、選択したすべてのデータセットで最も一般的な2つの設定（つまり、5フィルターと10フィルター）を使用します。データセットの詳細な統計を表1にまとめています。K-fiterはK-coreとは異なることに注意してください。前者は、ユーザーとアイテムが1回のパスでK未満のインタラクションでのみフィルタリングされることを意味します。対照的に、後者は、すべてのユーザーとアイテムが少なくともK個のインタラクションを持つまで再帰フィルターを示します

### 比較ベースライン
収集された論文から観察されるように、比較されたベースラインは異なる論文で大きく異なります。図1（c）に、これらの論文の上位10の広く比較されたベースラインを示します。これは、合計で93％の論文をカバーしています。つまり、93％の論文が10のベースラインの少なくとも1つを考慮しています。ベースラインは、大きく3つのカテゴリに分類されます。
1. メモリベースの方法（MM）：MostPop、ItemKNN [29]
2. 潜在因子ベースの方法（LFM）：BPRMF [25]、FM [24]、WRMF [17]、PureSVD [6]、SLIM[22]およびeALS[13]
3. 表現学習ベースの方法（RLM）：NeuMF[14]およびCKE[43]
この調査では、図1（c）で赤で強調表示されているように、7つのベースラインを考慮に入れています。特に、2つのMMが考慮されます。MostPopはパーソナライズされていない方法であり、すべてのユーザーに最も人気のあるアイテムを推奨します。 ItemKNN1は、アイテムの類似性に基づいてアイテムを推奨するK最近傍ベースの方法です。 [17]に続く暗黙のフィードバックデータに適合させ、コサイン類似性を採用します。 LFMに関しては、行列因数分解法の代表としてBPRMFを選択し、WRMFとeALSは将来の調査のために残します。 BPRFM（因数分解マシン）は、入力間の2次の特徴の相互作用を考慮します。 BPR損失を最適化することによってトレーニングします[25]。 PureSVDは、ユーザーアイテムの暗黙的な相互作用行列に対して従来の特異値分解を直接実行します。この場合、観測されないすべてのエントリは0に設定されます。 SLIM [22]は、制約された再構成二乗損失を最小化することにより、まばらなアイテム-アイテム類似性行列を学習します。 RLMに関しては、最先端のニューラルネットワーク手法であるNeuMF[14]が検討されています。 CKEは、ユーザーとアイテムのインタラクションデータに加えて、テキストと視覚の情報を含むため、今後の調​​査のために残されています。さらに、NeuFM [15]、VAE [19]、CDAE [40]などのより高度なディープラーニングベースラインも、将来の調査のために残されています。

### 目的関数
ポイントワイズ（収集された論文の49％）とペアワイズ（収集された論文の42％）の2種類の目的関数が広く利用されています。前者は、個々の好みの予測の正確さにのみ依存しています。後者は、アイテムのペアの予測の相対的な順序を考慮して、ランキングの損失を概算します。どちらが展開されているかに関係なく、モデル内で観察されていないフィードバックを適切に活用することが重要です。観察されたフィードバックを考慮するだけでは、フィードバックがランダムに欠落していないという事実を説明できないため、トップNの推奨者には適していません[40 ]。推奨タスクの目的関数をLdenoteとすると、ポイントワイズとペアワイズの目的は次のように与えられます。 （u、j）| ru j = 0}観測されたユーザー項目セットに加えて、O + = {（u、i）| rui = 1}; f（・）は損失関数です。 rui、ˆruiは、それぞれ項目iに対するユーザーuの観測および推定フィードバックです。 （u、i、j）は、ユーザーuが負の項目jよりも正の項目iを好むという三重の意味です。 rui j = rui −ru j、ˆrui j = ˆrui −ˆru j; Ω（Θ）は正則化項です。 Θは、学習するモデルパラメータのセットです。に関して。損失関数f（・）、ポイントワイズ目標は通常、二乗損失とクロスエントロピー（CE）損失を採用しますが、ペアワイズ目標は一般に対数損失とヒンジ損失を採用します:(）
多数派の研究[14、31、36、37]に従い、観察されなかったフィードバックを負のフィードバックとして扱います。観察されていないフィードバック[46]の背後には、さまざまな説明がある可能性があることに注意してください。表2（a）は、BPRMF、BPRFM、NeuMF、およびSLIMで使用される元の目的関数を示しています（GitHubリポジトリの追加資料の式を参照）。さらに、それぞれの影響をさらに調査するために、これらのベースラインでさまざまな目的関数を変更します。 MostPop、ItemKNN、PureSVDには目的関数がないことに注意してください。問題のランク付けよりも予測タスクの評価に適しているため、二乗損失は考慮しませんでした[25]。また、SLIMは複雑性が高く、スケーラビリティが低いため、さまざまな目標がSLIMに与える影響については調査しませんでした。これについては、セクション3.6で説明します。

### ネガティブサンプリング
セクション2.2.4で指摘されているように、観察されないフィードバック（否定的なサンプルとして扱われる[14、37]）を適切に活用することは、ユーザーの相対的な好みを学習するのに役立ち、より正確なトップNの推奨に役立ちます。これは、収集された論文の70％が、ポイント単位およびペア単位の関数に関係なく、目的関数を設計するときに観察されないフィードバックを考慮するという事実によってさらにサポートされます。ただし、ほとんどのユーザーは少数のアイテムに対してのみフィードバックを提供するため、観察されていないすべてのフィードバックを大量に活用することは実用的ではありません。したがって、効率と有効性のバランスをとるために、負のサンプリングが採用されています。通常、さまざまな種類のネガティブサンプリング戦略があります。たとえば、各ユーザーの観察されていないすべてのアイテムが同じ確率でサンプリングされる均一サンプラー[14]は、私たちの観察に関するコレクションのほぼすべての論文で採用されています。ネガティブサンプリングの影響をよりよく研究するために、推奨事項[13、41]でも検討されているアイテム人気ベースのサンプラーをさらに検討および比較します。（1）低人気サンプラー：各ユーザーに対して、彼女の観察されていないアイテム人気の低いものは、より高い確率でサンプリングされます。これは、ユーザーがあまり人気のないアイテムを好む可能性が低いという仮定に基づいています。 （2）人気の高いサンプラー：人気の低いサンプラーとは逆であり、人気の高い各ユーザーの観察されていないアイテムがサンプリングされる可能性が高くなります。背後にある理論的根拠は、ユーザーが多数のユーザーに好まれる非常に人気のあるアイテムにフィードバックを提供しない場合、それは彼女が実際にはこのアイテムに興味がない可能性があることを示しています。

### データ分割方法
収集された論文には、主に2種類のデータ分割方法があります。比率による分割（論文の61％）とリーブワンアウト（論文の28％）です。特に、比率による分割とは、データセット（つまり、ユーザーとアイテムの相互作用レコード）の比率ρ（たとえば、ρ= 80％）がトレーニングセットとして扱われ、残り（1 −ρ = 20％）がトレーニングセットとして扱われることを意味します。テストセットとして。 Leave-one-outは各ユーザーのレコードを指しますが、テスト用に1つのレコードのみが保持され、残りはトレーニング用に保持されます。さらに、論文の5％は、トレーニングセットとテストセットを固定タイムスタンプで直接分割します。つまり、固定タイムスタンプより前のデータがトレーニングセットとして使用され、残りはテストセットとして使用されます。論文の6％は、データ分割方法を報告していません。論文の61％は比率による分割を採用していますが、次の理由によりかなり異なります。（1）比率の設定が異なる、たとえば、ρ= 50％、70％、90％。 （2）グローバルレベルまたはユーザーレベルの分割。つまり、ユーザーの違いに関係なく、レコード全体をトレーニングセットとテストセットにグローバルに分割するものもあります。他の人はユーザーベースでトレーニングセットとテストセットを分割します。 （3）ランダムまたは時間認識の分割。比率による分割を利用している論文の中で、88％の論文は単にデータをランダムに分割しているのに対し、12％の論文はタイムスタンプに基づいてデータを分割しています。つまり、トレーニングとしての以前の（たとえば、ρ= 80％）レコードとテストとして後のもの。 Leave-one-outに関しては、分割は通常ユーザーベースで行われ、タイムスタンプはLeave-one-outのある紙の54％で考慮されていることがわかります。さらに、テストの効率を向上させるために、通常、各ユーザーが操作しない多数の否定的な項目（neд_test = 99,100,999,1,000など）をランダムにサンプリングし、各テスト項目を（neд_test +1）項目の中でランク付けします[14 ]。私たちの研究では、大多数の慣行に従い、データ分割方法としてρ= 80％を使用して、グローバルレベルでランダムおよび時間認識の比率による分割を選択し、将来として探索を1つに残します仕事。一方、テストプロセスをスピードアップするために、各ユーザーのネガティブアイテムをランダムにサンプリングして、テスト候補が1,000になるようにし、すべてのテストアイテムを1,000アイテムの中でランク付けします。表2（b）は、オリジン、5および10フィルター設定全体の6つのデータセットにおける各ユーザーの平均テスト項目数を示しています。ここで、すべての値は100未満であり、1,000のテスト候補が推薦者。

### 評価指標
評価指標は、コレクション内のさまざまな論文で大きく変化します。 図1（d）は、使用された評価指標の人気を示しています。 したがって、収集された論文の94％をカバーする上位6つの指標を採用します。つまり、これらの論文の94％が6つの指標の少なくとも1つを採用します。 それらは、適合率、再現率、平均平均適合率（MAP）、ヒット率（HR）、平均相互ランク（MRR）、および正規化された割引累積ゲイン（NDCG）であり、最初の4つのメトリックはテスト項目が上位に存在するかどうかを測定します。 N推奨リスト、後者の2つのメトリックは、テスト項目のランキング位置を説明します（GitHubリポジトリの追加資料の式を参照）

### ハイパーパラメータの調整
パラメータの検証や検索戦略を含むハイパーパラメータの調整は、推奨者のトレーニングにおいて重要な役割を果たし、最終的なパフォーマンスに影響を与えます。検証戦略。分析を通じて、37％以上の論文が、テストセットのパフォーマンスに基づいてハイパーパラメータを直接調整していることがわかりました。つまり、同じデータを使用してモデルパラメータを調整し、モデルのパフォーマンスを評価します。したがって、情報がモデルに漏れて、履歴データに過剰適合する可能性があります。実際、交差検定でのトレーニングセットとテストセットの分割に加えて、ネストされた検証と呼ばれるハイパーパラメーターの調整に役立つ追加の検証セットを保持する必要があります。ネストされた検証では、モデルが検証セットで最高のパフォーマンスを達成したときに、最適なハイパーパラメータ設定が取得されます。そうすることで、モデルのトレーニングと評価のプロセスで情報漏えいの問題を十分に回避できます。私たちの研究では、ネストされた検証戦略を採用しています。つまり、相互検証の各フォールドで、ハイパーパラメーターを調整するための検証セットとして、トレーニングセットからレコードの10％をさらに選択します。ハイパーパラメータの最適な設定が決定したら、トレーニングセット全体（検証セットを含む）をフィードして最終モデルをトレーニングし、テストセットのパフォーマンスを報告します。特定のベースラインの計算要件のため、検証セットをトレーニングセットから分割するための相互検証の内部ループのハイパーパラメーター空間を妥当な時間内に検索できませんでした。検索戦略。私たちの観察から、収集されたほとんどすべての論文は、最適なパラメータ設定を見つけるためにグリッド検索[14、37]を採用しています。特に、各ハイパーパラメータには、事前知識に基づいて可能な値のセット（つまり、検索空間）が提供され、検索空間全体をトラバースすることによって最適な設定が取得されます。モデルにm個のパラメーターがあり、各パラメーターにn個の可能な値の平均があるとすると、すべてのパラメーターの最適な設定を見つけるために、モデルをnm回実行する必要があります。したがって、グリッド検索は、パラメーターが少ないモデルに適しています。そうしないと、組み合わせ爆発の問題が発生する可能性があります。パラメータ調整効率を改善するために、他の戦略が導入されました。各パラメーターの検索スペースが与えられると、ランダム検索[4]は、検索スペース全体をトラバースするのではなく、事前定義された時間（たとえば、30）の試行をランダムに選択します。計算時間のごく一部の範囲内で、同じくらい良いまたはわずかに悪いモデルを見つけることができます。それどころか、ベイジアンHyperOpt [30]はブルートフォースではなく、グリッドおよびランダム検索と比較してよりインテリジェントな手法です。これは、過去の試行からの情報を利用して、結果の品質を損なうことなく、探索する次のハイパーパラメータのセットを通知します[7]。効率的なハイパーパラメータ調整を実現するために、ベイジアンHyperOptを採用して、NDCGでハイパーパラメータ最適化を実行します。これは、図1（d）に示すように、すべての評価メトリックの中で最も人気のあるメトリックです。この場合、他のメトリックは、NDCGでの最適な結果と同時に最適化されることが期待されます。

## さまざまな要因の影響
### データ前処理前処理
戦略(オリジン、5および10フィルター）の影響を研究するために、ベイジアンHyperOptを採用してハイパーパラメーター最適化を実行します。 30トレイルの各データセットの各戦略の下での各ベースラインのNDCG@10[7]。各ベースラインの元の目的関数を維持し（表2（a）を参照）、均一なサンプラーを採用し、データ分割方法としてグローバルレベル（ρ= 80％）での時間認識比率による分割を採用します。さらに、最新のトレーニングセットの10％が、ハイパーパラメータを調整するための検証セットとして保持されます。ハイパーパラメータの最適な設定が決定したら、トレーニングセット全体をフィードして最終モデルをトレーニングし、テストセットのパフォーマンスを報告します（これ以上の説明はありませんが、以降の調査ではこれらの設定に従います）。図2は、最終結果を示しています。SLIMは計算が非常に複雑であるため省略されており、妥当な時間内に完了することができません（セクション3.6を参照）。一方、原点設定を使用したAMZeでのNeuMFの結果は、計算メモリが不足しているため利用できません。スペースに限りがあるため、結果はNDCG@10でのみ報告します。全体として、結果から3つの異なる傾向を観察できます。（1）異なるベースラインのパフォーマンスは、さまざまな設定のML-1Mで比較的安定しています。 （2）Yelp、Book-X、AMZeでは、すべてのベースラインのパフォーマンスが一般的に上昇します。 （3）LastfmとEpinionsで明らかなパフォーマンスの低下が見られます。最も可能性の高い説明は、表1に示すように、データセットの密度は増加しますが（origin→5-filter→10-filter）、各ユーザーのトレーニングセットの平均長はML-1Mで安定していることです（86）。 Yelp、Book-X、AMZeで増加。表2（b）に示すように、Lastfm（39→30→27）とEpinions（35→23→20）で減少します。ユーザーあたりのトレーニングデータが多いほど、モデルのトレーニングが向上します。つまり、より正確なパフォーマンスを実現でき、その逆も可能です。さまざまなベースラインのパフォーマンスに関して、（1）ほとんどの場合、MM – MostPopのパ​​フォーマンスが最悪であり、推奨におけるパーソナライズの重要性を示唆しています。 ItemKNNはLFMとDLMに打ち負かされており、効果的な推奨におけるLFMとDLMの優位性を示しています。ただし、ML-1Mでは、MostPopのパ​​フォーマンスはItemKNN、PureSVD、さらにはBPRMFのパフォーマンスを上回り、効果的な推奨で人気がある可能性を示しています。また、Lastfmでは、ItemKNNはLFMやDLMと比較して同等のさらに優れたパフォーマンスを実現します。これは、近隣ベースのアイデアは単純ですが、LFMとDLMによって吸収され、推奨精度をさらに向上させることができることを意味します[18]。 （2）w.r.t. 3つのLFMであるBPRMFは、通常、PureSVDよりもパフォーマンスが優れていますが、BPRFMよりもパフォーマンスが劣ります。 PureSVDは単純ですが、ユーザーアイテムの暗黙的な相互作用行列に従来の特異値分解を直接適用しますが、BPRMFおよびBPRFMと比較した場合に同等のさらに優れたパフォーマンスを達成できる場合があります（Lasfm-origin、Yelp-10-filter、Bookを参照） -X-10-フィルター）; （3）NeuMFは、すべてのベースラインの中で唯一のDLMとして、BPRFMと同等のパフォーマンスを発揮し、ML-1MおよびAMZeのBPRMFよりも優れています。ただし、残りの4つのデータセットでは、一般にBPRFM、BPRMF、さらにはPureSVDよりもパフォーマンスが低くなります。これは、DLMが適切に調整されたパラメーターを使用する従来の方法よりも常に優れているとは限らないという以前の調査結果[7]と一致していますが、表3で検証されているように、ほとんどの場合、トレーニングにはるかに多くの費用がかかります。

### 目的関数
さまざまな目的関数の影響を調べるために、セクション3.1の10フィルター設定で見つかったベースラインに最適なパラメーターを採用し、BPRMF、BPRFM、およびNeuMFの目的関数のみを変更します（これ以上の説明はありません。研究は10フィルター設定に基づいており、セクション3.1）にある対応する最適パラメーターを採用しています。結果を図3に示します。ここで、Poi + CL（ポイントワイズクロスエントロピー損失）、Pai + LL（ペアワイズログ損失）、Pai + HL（ペアワイズヒンジ損失）は、Lpoi + fcl、Lpai+に対応します。表2（a）のそれぞれfllとLpai+fhl。いくつかの結論を導き出すことができます。（1）全体として、Pai + LLは一般に6つのデータセットで最高のパフォーマンスを達成しますが、Poi+CLとPai+HLのパフォーマンスを比較することは困難です。たとえば、Poi+CLはAMZeではPai+HLよりも優れていますが、他のデータセットではケースが異なります。 （2）さまざまなベースラインの観点から、BPRMFは通常Pai+LLで最高のパフォーマンスを達成します。 BPRFMは、さまざまな目的に対する感度が比較的低く、その堅牢性を示しています。 NeuMFはPoi+CLおよびPai+LLと同等のパフォーマンスを発揮しますが、Pai+HLではより悪い結果が得られます。

### ネガティブサンプリング
このサブセクションでは、10個のフィルター設定を使用した6つのデータセット全体で、さまざまなネガティブサンプラー（均一、低人気（L-pop）、高人気（H-pop））がBPRMF、BPRFM、NeuMFに与える影響について説明します。この目的のために、他のパラメーターを固定したまま、ベースラインのネガティブサンプラーのみを変更します。要約すると、図4に示すように、均一サンプラーは単純ですが、最高のパフォーマンスを実現します。特に、直感に反する観察結果が観察されます。均一サンプラーのベースラインは、ローポップのベースラインよりも優れています。直感的には、ユーザーは人気の低いアイテムを購入する傾向がない可能性があります。つまり、人気の低いアイテムは、ユーザーにとってネガティブなアイテムである可能性が高くなります。しかし、それは経験的な結果によって覆されています。また、L-popはH-popをわずかに上回っています。これは、一般的に人気のあるアイテムは、人気のないアイテムよりもネガティブなアイテムになる可能性が低いことを示しています。一方、ML-1Mでは、BPRMFのH-popのパフォーマンスがUniformおよびL-popのパフォーマンスをはるかに上回っていることにも気付きました。これは、ユニフォームと人気ベースのサンプラーを適切に組み合わせることで、推奨精度が向上する可能性があることを示しています[16]。

### データ分割方法
ここでは、さまざまなデータ分割方法が推奨パフォーマンスに与える影響をテストすることを目的としています。実際の研究では、グローバルレベルでのランダムおよび時間認識の比率による分割のみをρ= 80％と比較し、他のデータ分割方法（Leave-one-outなど）を将来の作業として残します。図5（a-d）は、10個のフィルター設定を使用した4つのデータセットの7つのベースラインの結果を示しています。ここでは、ランダムを意識した比率による分割を使用したベースラインが、時間を意識した比率による分割を使用したベースラインよりも優れていることがはっきりとわかります。 Epinionsで。背後にある理由は、ランダム認識分割と比較して、時間認識分割はトレーニングとテストデータのパターンに強い制約を課し、トレーニングの難易度を高めるためです。ただし、これは、履歴から将来を推測しようとする実際の推奨シナリオにより近いものです。私たちの研究はまた、ランダムを意識した比率による分割を使用した以前の研究で開示された経験的結果が、実際のシナリオの結果と比較して過大評価されている可能性があることを示唆しています。

### 評価指標
セクション3.1で説明したように、ベイジアンHyperOptを採用して、NDCGの最適化を介して30回の試行のハイパーパラメーター最適化を実行します。ただし、この調査では、適合率、再現率、HR、MAP、MRR、NDCGを含む6つの指標が使用されています。最適なNDCGに最適なハイパーパラメータ設定は、必ずしも最適なw.r.tを保証するものではありません。他の5つのメトリック。したがって、それぞれの最適化が達成されたときのさまざまなメトリックの相関関係を調査します。特に、10フィルター設定の各データセットのベースラインごとに、ベイジアンHyperOptは30の証跡を実行します。したがって、ベースラインの検証パフォーマンスに対応する30のエントリがあり、各エントリには6つのメトリックの結果が含まれます。精度：0.24;リコール：0.07; HR：0.57; MAP：0.17; MRR：0.76; NDCG：0.42]。 6つのメトリックの最適な結果が同時に達成されない可能性があるため、各メトリックの30のエントリから最適なものを選択し、最終的に6つのエントリを取得します。各エントリは、対応するメトリックで最良の結果を記録します。これに基づいて、ペアごとに計算し、それらのいずれか2つ（NDCGとHRなど）がエントリごとに同時に最良の結果を達成できる時間を記録します。たとえば、NDCGの最適なエントリが与えられた場合、このエントリの残りの5つのメトリック（HRなど）が最適かどうかを確認します。はいの場合、相関行列の対応する位置（NDCG、HR）に1つ追加します。それ以外の場合は0。同じルールが他の5つのメトリックの最適なエントリに適用されます。 MostPopを除いて、ハイパーパラメータがないため、6つのデータセット（6 * 6 = 36）全体で6つのベースラインの結果を累積し、最終的に図5（e）に示すような相関行列を取得します。は[0,1]（36で除算）の範囲に正規化され、色が濃いほど相関が強いことを示します。つまり、2つのメトリックがその間に最良の結果を達成する可能性が高くなります。結果は、最適なNDCGの最適なハイパーパラメータ設定では、他の5つのメトリックすべてに対して最適な結果を保証できないという私たちの主張を検証するのに役立ちます。さらに、相関行列が非対称であることがわかります。たとえば、（NDCG、HR、0.72）の相関は（HR、NDCG、0.64）よりも高くなります。つまり、最高のNDCGを備えたモデルが最高のHRを達成する確率は、最高のHRを備えたモデルが最適なNDCGを獲得する確率よりも高くなります。一方、最良のMRRおよびMAPは、（MRR、MAP、0.78）および（MAP、MRR、0.75）と同時に保証されやすくなります。さらに、6つのデータセットにわたる7つのベースラインでの推奨パフォーマンスを示すという観点から、メトリック間のケンドールの相関[35]を調べます。結果は図5（f）に示されています。ここで、色が濃い（相関が強い）ということは、メトリックがより同一のランキングを生成することを意味します。 Recallは他のメトリックとの相関が著しく低いのに対し、残りのメトリックによって生成されたランキングはかなり強い相関を示しています。要するに、説得力のある確かな評価は、w.r.tで実行されるべきです。より多様なメトリック。

### 複雑さの分析
表3は、6つのデータセットの7つのベースラインのトレーニング時間（ベイジアンHyperOptによって10フィルター設定で検出された最適なハイパーパラメーターを使用）を示しています。すべての実験は、8つのNVIDIATeslaK80アクセラレータを備えたAmazonEC2P2インスタンス（p2.8xlarge）で実行され、それぞれがNVIDIA GK210 GPUのペアを実行し、12ギブのメモリを提供します。 488GiBメモリを共有する16CPUコア（2.3 GHZ）。いくつかの主要な調査結果が記載されています。 
（1）MostPopは、計算された人気によってすべてのアイテムをランク付けするだけなので、トレーニングで最速のものです。 
（2）PureSVDは、時間の複雑さO（min {m2 f、n2 f}）の次点です。ここで、m、n、fは、それぞれユーザー、アイテム、および特異値の数です。他のLFMおよびDLMと比較して、時間計算量と精度の間のより良いバランスを実現します。特に、図2に示すように、BPRMFと同等に、場合によってはさらに優れたパフォーマンスを発揮しますが、トレーニング時間はBPRMFの数千分の1です。 
（3）ItemKNNは10フィルター設定のすべてのベースラインの中で3番目にランク付けされていますが、時間計算量O（mn2）のため、時間コストは原点設定とともに2倍に増加します。さらに、類似性マトリックスも膨大なメモリを消費します。たとえば、元のAMZe（n≈106）では、類似性マトリックスを保存するのに（64ビット∗ 106 ∗ 106）/ 1012=64Tのコストがかかります。この問題を緩和するために、メモリ内の各ターゲットアイテムの上位100個の類似アイテムのみを保持します。 
（4）BPRFMのトレーニング時間は、ユーザーとアイテムの潜在要因に加えて、グローバル、ユーザー、アイテムのバイアスを更新する必要があるため、BPRMFのトレーニング時間よりもわずかに長くなります。両方の方法の時間計算量はO（| R | d）です。ここで、Rは観測されたフィードバックの総数であり、dは潜在因子の次元です。
 （5）ItemKNNと同様に、10フィルター設定のSLIMの時間コストは許容範囲内ですが、時間計算量O（| R | n）のため、原点設定とともに大幅に増加します。一方、学習したアイテムの類似性マトリックスのために、メモリコストの大きな問題も発生します。したがって、ItemKNNとSLIMはどちらも、大規模なデータセットに対してスケーラブルではありません。 
 （6）NeuMFは同等のパフォーマンスをもたらしますが、特に大規模なデータセットでは、BPRFMよりもはるかに多くのトレーニング時間を要します。たとえば、AMZeでは、NeuMFのトレーニング時間はBPRFMのトレーニング時間の10倍です。

## ベンチマーク評価
### DaisyRecの概要
私たちの研究をサポートするために、DaisyRecという名前のPythonツールキットを作成します。Daisyは「レコメンダーシステムの多次元fAIrlycompArIson」の略です。さまざまな最先端のレコメンダーを再現することを主な目的とする既存のオープンソースライブラリ（LibRec [11]、OpenRec [42]、DeepRec [45]など）とは異なり、DaisyRecは推奨事項で厳密な評価を実行します。これは、広く使用されているディープラーニングフレームワークPytorch（pytorch.org/）に基づいて構築されており、図6は、ローダー、リコメンダー、エバリュエーターの3つのモジュールで構成される全体的な構造を示しています。特に、ローダーは主に次のことを目的としています。（1）データセットをロードして前処理する。 （2）選択したスプリッターに基づいてトレーニングセットとテストセットに分割します。 （3）ステップ2に従って適切なスプリッターを選択することにより、検証セットをトレーニングセットから分割します。 （4）サンプラーを選択してトレーニング用のネガティブアイテムをサンプリングします。 （5）推奨者に合うようにデータを特定の形式に変換します。 2つのコンポーネントがRecommenderに含まれており、「Algorithm」は、MM（MostPopやItemKNNなど）、LFM（MF、PureSVD、SLIM、FMなど）、RLMなどの最先端の機能を実装しています。 （例：Item2Vec、MLP、NeuMF）; LossSelectorを使用すると、アルゴリズムのさまざまな目的関数を柔軟に変更できます。 EvaluatorにはTunerとMetricが装備されており、前者はハイパーパラメータの最適化を実現し、後者はPrecisionなどの従来のランキングメトリックを実装します。要約すると、DaisyRecのすべてのモジュールは、ユーザーが展開しやすいようにラップされており、この拡張可能で適応性のあるフレームワークに新しいアルゴリズムを簡単に追加できます。私たちはDaisyRecを最新の状態に保ち、更新されたバージョンはまもなくリリースされます。ここでは、いくつかの新しい影響力のある要素が追加されます。たとえば、（1）異なるパラメーターの初期化方法。 （2）異なる正則化用語（L1およびL2ノルムなど）。 （3）異なる予測関数、たとえば、類似性と負のユークリッド距離を考慮します（4）より客観的な関数、たとえば、トップ1の損失[16]; （5）より高度なベースライン（NeuFM [15]、CDAE [40]、VAE [19]など）。

### 標準化された手順
セクション2は、セクション3で経験的に分析された推奨評価の重要な要素を示しています。厳密な評価を達成するために、一連の標準化された手順を提案し、それに応じてすべての研究者の努力を呼びかけ、標準化を効果的に強化することを目指します。推薦評価の。 
（1）各ドメインをカバーするすべての公開データセットで推奨を評価することは不可能です。ただし、特にプライベートデータセットで評価された論文（たとえば、営利企業からの機密データ）については、セクション2で説明した少なくとも1つの広く使用されているデータセットを検討する必要があります。そうでなければ、結果はその後の研究で簡単に再現することができませんでした。 
（2）セクション3.1は、さまざまなデータ前処理戦略がパフォーマンスに影響を与えることを確認します。元のデータセットに加えて、データの希薄性の問題を緩和するために5および10のフィルター設定が推奨され、データの前処理の詳細に関する明確な説明が不可欠です。 
（3）セクション2.2.3のさまざまなタイプ（MM、LFM、およびDLM）のベースラインを選択し、比較することをお勧めします。セクション3.1に示すように、さまざまなタイプのベースラインのパフォーマンスは、さまざまなシナリオで大きく異なります。つまり、MM（MostPopなど）と単純なLFM（PureSVDなど）は、DLM（NeuMFなど）よりもパフォーマンスが優れている場合があります。比較されるベースラインが多ければ多いほど、評価はより包括的で信頼性が高くなります。 
（4）パフォーマンスは、セクション3.2で経験的に証明されているように、さまざまな目的関数の影響を受けます。公平に比較​​するには、さまざまなタイプの目的関数を使用してすべてのメソッドを評価し、包括的な評価を行うことをお勧めします。これにより、提案されたメソッドの貢献度をより適切に位置付けることができます。 
（5）異なるネガティブサンプリング戦略を提案または研究することを目的とした論文を除いて、比較されるすべての方法は同じネガティブサンプラーを採用する必要があります。 
（6）データ分割方法については、時間対応の比率分割と時間対応のリーブワンアウトの両方をお勧めします。タイムスタンプを使用すると、実際の推奨シナリオがより適切にシミュレートされます。に関して。比率で分割すると、グローバルレベルとユーザーレベルの両方が適切に機能し、より実現可能で便利な比較のために、ρ= 80％が推奨されます。 
（7）セクション2.2.7の6つのメトリックのうち少なくとも2つを採用する必要があります。一方（たとえば、精度）は、テスト項目がトップN推奨リストに存在するかどうかを測定し、もう一方（たとえば、NDCG）は）推奨アイテムのランキング位置を測定します。 
（8）W.r.t.ハイパーパラメータ調整、ネストされた検証は必須です。つまり、検証セットとして部分的な（たとえば、10％）トレーニングデータを保持します。よりインテリジェントなパラメータ検索戦略として、ベイジアンHyperOptが推奨され、ベースラインの共有パラメータの検索スペースは同じに保つ必要があります。トレイルの数（[7]に従って30に設定）は、パフォーマンスをさらに向上させるために増やすことができます。最適なパラメータ設定は、再現のために十分に報告する必要があります。
 （9）コードとデータセットは複製に利用できる必要があります[23]。会議の会場は、それらを必需品として作成し、品質を測定し、会議中に受け入れられた各論文と一緒に短いコードのデモンストレーションを要求することさえできます。

### ベースラインのパフォーマンス
公正な比較のためのより良い参照を提供することを目的として、表4は、N=10の10フィルター設定での6つのデータセットの6つのメトリックにわたる7つのベースラインのパフォーマンスを示しています。 スペースの制限により、他の結果（N = 1,10,5,20,30,50の原点と5フィルター設定など）はGitHubにあります。 すべての最適なハイパーパラメーターは、ベイジアンHyperOptによって検出され、30回の試行でNDCG @ 10を最適化します（セクション3.1を参照）。詳細なパラメーター設定は、GitHubの追加資料にあります。 セクション3.1で説明した同様の観察結果に注目することができます。 具体的には、BPRFMはML-1MおよびAMZeで最高のパフォーマンスを実現します。 SLIMはLastfmで最高のパフォーマンスを発揮します。 NeuMFはEpinionsの勝者です。 Yelpの最高のパフォーマンスは、BPRFMw.r.tによって得られます。 （適合率、再現率、HR、NDCG）およびPureSVD w.r.t. （MAP、MRR）; Book-Xを使用している間、BPRMFは最高のパフォーマンスを実現します。 （適合率、再現率、HR、NDCG）、それでもPureSVDは最高のMAPとMRRに到達するのに役立ちます。

## 関連研究
科学的発見の重要な特徴として長い間認識されてきましたが、再現性は最近ますます危機として特徴付けられています[1,21]。これは、アーティファクトレビューとバッジ2に関する最近開発されたACMポリシーと、セミナー[10]、ワークショップ[5]、主要な会議での焦点を絞ったトラック[12]などの新たな取り組みによって証明されるように、コンピューターと情報科学の主要な関心事になりつつあります。レコメンダーシステムの研究に固有の議論は、新しく提案された方法とベースラインの方法との比較の公平性に集中してきました[7、26]。非常に最近の研究では、Dacremaetal。 [7]神経モデルは、微調整された記憶および潜在因子ベースの方法をほとんど上回ることができないことを発見しました。これは、Rendleらによっても発見された同様の発見です。 [26]。重要性にもかかわらず、推奨システムの研究における再現性の向上は、推奨性能の多くの影響力のある評価要素のために非常に困難です。サイードら。 [27]さまざまな実装フレームワーク間、および評価データセットとメトリック間で、推奨方法の有効性に大きな違いが見られます。コンパニオンツールキットRiVal[28]がリリースされ、データ分割と評価メトリックの制御が可能になりました。 Beeletal。 [3]ニュースや研究論文の推奨で同様の現象を見つけ、ユーザーの特性や推奨の時間などの影響力のある要因を特定します。 Valcarceetal。 [35]具体的には、アイテムのランク付けの評価指標の特性を研究し、精度を最も堅牢で、NDCGが最高の識別力を示すものとしてマークします。最近では、Rendleetal。 [26]は、マトリックス因数分解などのベースライン手法におけるハイパーパラメータ検索の重要性を示し、公正な比較のために手法を大幅に調整する必要がある標準化されたベンチマークの必要性を強調しています。

ただし、既存のベンチマークは、プレニューラル手法[27]、単一の評価係数[35]、または推奨問題を定式化する方法として推奨されていない評価予測[26]のいずれかに制限されています[20]。 さらに、既存のすべてのベンチマークは、2つまたは3つのデータセット（[7]を含む）を考慮し、新しく公開された作品によってしばしば選択される利用可能なデータセットの豊富さを無視します。 評価問題の完全な処理を目的として、私たちの仕事は、重要な評価要素（たとえば、既存のベンチマークで欠落している目的関数や負のサンプリング）を検索するために、大量の文献を分析するボトムアップアプローチを採用しています。 将来の研究のための強力な基盤を築くことを期待して、これまで以上に大規模な実証的研究によって裏付けられたベンチマークを提示します。  

## 結論
効果的なベンチマークがないため、再現不可能な評価と不公平な比較が推奨の2つの主要な懸念事項になっています。したがって、このペーパーは、再現性のある評価と公正な比較のためのベンチマーク推奨を目的としています。この目的のために、8つのトップティア会議から最近3年間（2017-2019）に発行された85の推奨論文が体系的にレビューされ、データ分割方法、評価メトリック、ハイパーパラメータなどの評価に関連する重要な要素が要約されています。調整戦略など。広範な実証的研究を通じて、評価に対するさまざまな要因の影響が包括的に分析されます。したがって、標準化された手順を提案し、後の研究の参照として、6つのメトリックにわたって広く使用されている6つのデータセットに対して、7つの適切に調整された最先端のアルゴリズムのパフォーマンスを提供することにより、厳密な評価のためのベンチマークを作成します。最後に、ユーザーフレンドリーなPythonツールキットであるDaisyRecが、厳密な評価を推奨するという観点からリリースされました。今後の作業のために、たとえば、他の場所からより多くのベースラインを調査し、より多様な推奨タスク（セッション対応など）に飛び込むことによって、調査を深める予定です。